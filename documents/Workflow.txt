Below is a comprehensive map of every part of the repository that is involved in pulling information out of documents, structuring it, learning from it or otherwise â€œteachingâ€ the system over time.
Wherever possible Iâ€™ve linked the component to the files you can open and the tables it writes into â€“ this should help you trace a piece of knowledge from raw bytes all the way through to an agent decision or a trained model.

1â€¯â€“â€¯Document Ingestion & Text Extraction
These utilities turn arbitrary files into plainâ€‘text that can later be analysed.

ingestion.py â€“ OCR, PDF/DOCX/html/text extraction; raises DocumentIngestionError on failure.
classification.py â€“ wraps extract_text, then calls an LLM (via Ollama) to
â€¢ classify documents into legal categories
â€¢ suggest a normalized filename
â€¢ record the results + extracted entities in the SQLite database.
ğŸ“¦ Database tables populated: files_index (via FileIndexRepository), classifications, entities.

2â€¯â€“â€¯Categorisation & LLMâ€‘powered Analysis
Classification prompt and postâ€‘processing in classification.py.
Agent routes for analysis & classification (analysis.py) delegate to AgentService tasks such as classify and extract_entities.
3â€¯â€“â€¯Entity & Pattern Extraction
Multiple overlapping subsystems extract structured data from text.

Patternâ€‘based (YAML/regex)

pattern_extraction_mixin.py provides extract_entities_with_patterns() andâ€¯â€¦_relationshipsâ€¦ using the loader in extraction_patterns.py (currently a placeholder).
Patterns are intended to be stored in YAML and reâ€‘loaded at runtime.
Hybrid/LLM extractors

Workflow tasks (operations.py) call a registered entity extractor agent that implements extract_entities() and returns details, confidence, provenance, etc.
The route above (/agents/entities) dispatches to that agent.
Database helpers

database.py exports extract_entities() (imported by the classification module) and stores entity rows in file_entities + links in file_entity_links.
Evidence clustering

evidence_clusterer.py tokenises text, produces sentence embeddings (SentenceTransformer) and groups them with Kâ€‘means.
Useful for semantic summarisation and â€œlearningâ€ salient clusters.
4â€¯â€“â€¯Embeddings & Vectorisation
Vector representations are central to search, memory and knowledgeâ€‘graph work.

Agentâ€‘side logic in
unified_embedding_agent.py
â€“ supports multiple models (Legalâ€‘BERT, Sentenceâ€‘Transformers, OpenAI, Jina, BGE, etc.), caches results, batches requests and optionally pushes nodes/edges into Memgraph.
â€“ configurable vector stores: FAISS, ChromaDB or a unified store from the service container.

HTTP endpoints
embedding.py exposes /api/embedding to call the embedding agent.

Database tables

file_content_chunks â†’ chunking text
file_chunk_embeddings stores JSON vectors + hashes
vector store backâ€‘ends under mem_db/vector_store/* and chroma/FAISS helpers in mem_db/memory/chroma_memory/....
Memory layer (see next section).

5â€¯â€“â€¯Shared Memory & Knowledge Base
This is where the application â€œremembersâ€ what it has seen and allows agents to share what theyâ€™ve learned.

UnifiedMemoryManager (unified_memory_manager.py)

SQLiteâ€‘backed memory_records table
Optional semantic search via ChromaDB or FAISS
Async API with namespace isolation
Caches, stats, thread/async safety
Used by agents through a memory mixin (memory_mixin.py) and service container.
Memory models & interfaces (mem_db/memory/models/*) define the shape of documents, personas, legal knowledge, etc.

Service integration and support for proposals in proposals_db.py and service_integration.py.

Knowledgeâ€‘centric tables in the main database:
manager_knowledge, knowledge_proposals, manager_questions, manager_personas, manager_skills, organization_proposals, plus related feedback/actions.

6â€¯â€“â€¯MLâ€‘Driven Optimization & Learning
Contained in ml_optimizer.py:

Tracks performance metrics (processing time, accuracy, API cost, token usage) in its own SQLite db.
Defines document features, parameters and composite scoring.
Optimization engine:
records history,
computes similarity between documents,
suggests new parameter sets,
caches results.
Threshold model training â€“ a logisticâ€‘regression classifier built from feedback to derive â€œautoâ€‘approve/review/rejectâ€ thresholds.
Methods for recording token usage and feedback, and for retraining on demand.
Exposed via a serviceâ€‘container factory; used by background jobs and healthâ€‘checks.
ğŸ‘‰ scikit-learn is an optional dependency; training code is guarded accordingly.

7â€¯â€“â€¯Agents, Taskmaster & Workflow
â€œLearningâ€ also occurs through autonomous agents and scheduled jobs.

production_manager orchestrates initialization, dispatching tasks (extraction, classification, organisation proposals, etc.) and handles degradation notices when models are missing.
Taskmaster (tables taskmaster_runs, taskmaster_tasks, taskmaster_job_queue, etc.) powers background workflows that can:
scan directories,
ingest documents,
generate organisation proposals,
autoâ€‘classify files,
update the memory store, etc.
Workflows can propose new knowledge or new organisation rules which are stored and later reviewed.

8â€¯â€“â€¯Database Schema (Knowledge storage)
Key tables involved in extraction/learning:

Table	Purpose
files_index	canonical registry of files
file_content_chunks	segmented text for embedding/search
file_chunk_embeddings	embedding vectors
file_entities / file_entity_links	extracted entities & relationships
document_analytics	derived metrics (word count, top terms)
manager_knowledge	curated knowledge items (term, category, description, confidence, statusâ€¦)
knowledge_proposals	proposed knowledge additions/changes
organization_proposals	file rename/move suggestions from agents
manager_questions	open questions for human review
feedback_records (ml_optimizer)	user review feedback used for learning
performance_records, optimization_cache	ML optimizer history
9â€¯â€“â€¯Configuration, Patterns & Rules
file_tagging_rules.json, integration_rules.json, organization_rules.json â€“ rule sets used during extraction and organisation.
extraction_patterns.py and its YAMLâ€‘based counterpart control regexâ€‘style knowledge extraction.
ml_optimizer.py uses config_models.MLOptimizerConfig to customise learning behaviour.
10â€¯â€“â€¯Supporting UI & Tools
GUI tabs (gui/tabs/â€¦) include embedding operations, entity viewers, proposal consoles.
org_console â€“ a small web UI for browsing and generating organisation/knowledge proposals.
Development notes and tests in tools/dev/â€¦ illustrate how classification/agents are exercised.
How the pieces fit
A file is discovered (watched directory/taskmaster) â†’ text extracted (utils/ingestion) â†’ optionally classified (utils/classification).
Entities/patterns are pulled out, saved to the DB and optionally linked.
Chunks are created, embeddings computed (via the embedding agent) and stored; these feed vector search in the unified memory manager.
Agents consult shared memory, generate proposals (organisation, knowledge, Q&A) and write back results.
ML Optimizer monitors all processing, records metrics and feedback, and periodically retrains threshold/parameter models to improve future runs.
Knowledge proposals/questions enable human review, and once approved they become permanent entries in manager_knowledge, effectively â€œteachingâ€ the system.
Summary
This applicationâ€™s knowledgeâ€‘extraction/learning ecosystem spans:

Lowâ€‘level data capture (ingestion, OCR, text/chunking)
Semantic understanding (classification, entity extraction, clustering, embeddings)
Persistent memory (SQLite + vector stores, shared across agents)
Adaptive behaviour (ML optimizer training, proposals/feedback loops)
Agent orchestration (production manager + taskmaster workflows)


Use this accuracy-first path only:

ğŸ“„ Processing
Add full case folder(s), then run âš¡ Process Documents.
Keep these ON: Extract Text, Extract Metadata, Analyze Content Structure, Generate Summary, Index to Vector Store, Enable OCR.
ğŸ” Entities
Run extraction with Ensemble Engine = Auto (Ensemble).
Keep Enable Knowledge Graph Grounding ON.
Approve high-confidence rows, then ğŸ“¤ Sync to Agent Memory.
Knowledge Graph
Paste key narrative text and run Extract KG from Text.
Review triples, then Import Extracted Triples.
Also run Ingest Documents â†’ Extract KG from Files for full coverage.
ğŸ§  Agent Memory Manager (most important for learning quality)
Work from the unified workspace.
Edit ontology/canonical/jurisdiction/provenance fields once.
Approve only validated proposals; reject weak ones.
Use Save Unified Selected/All after curation.
ğŸ“‚ Organization
Run âš¡ Generate Scoped only after curation is clean.
Manually review proposals.
ğŸšš Apply Approved last.
ğŸ”© Embeddings
Use Nomic v1.5 (High-Fidelity).
Process full curated corpus (folder mode) after steps 1â€“4.
Use this for retrieval/similarity support, not as source-of-truth extraction.
If you want, I can turn this into a strict runbook checklist you can execute case-by-case.