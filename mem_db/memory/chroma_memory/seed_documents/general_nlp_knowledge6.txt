Project Goal: To process, analyze, and extract insights from a corpus of 1,500 documents, leveraging NLP techniques and providing structured data outputs and visualizations. This includes sentiment analysis, named entity recognition (NER), document classification, and topic discovery.
1. Overview and Assumptions 
	► 1.1 Purpose:
		• Provide a scalable, modular, and user-friendly pipeline for analyzing large sets of textual documents.
		• Support tasks including sentiment analysis, named entity recognition (NER), document classification, topic modeling, and similarity analysis.
		• Generate structured data outputs (databases, CSV/JSON) and visualizations for reporting.
		
	► 1.2 Key Assumptions and Edge Cases:
		• Input Language: Text is assumed to be English, encoded in UTF-8. Mitigation: For multilingual support, integrate language-specific tokenizers and models (e.g., spaCy's language models, multilingual BERT).
		• Noisy Data: Data may contain HTML tags, special characters, or unrecognized tokens. Mitigation: Implement robust preprocessing with regular expressions, character normalization, and fallback logic (e.g., replacing unknown tokens with <UNK>).
		• Rare Entities: For domain-specific or infrequent entities, NER performance may be limited. Mitigation: Combine spaCy's statistical NER with rule-based matching (using spaCy's Matcher and PhraseMatcher). Consider fine-tuning a transformer model on domain-specific data if available. Explore entity linking to knowledge bases (Wikidata, DBpedia) for context and disambiguation.
		• Hardware: GPU support is available and highly recommended for transformer-based methods (sentence embeddings, fine-tuning). For extremely large datasets, consider distributed environments (e.g., Spark, Dask, or cloud-based solutions).
		• Document Length: Transformer models have maximum sequence length limitations (typically 512 tokens). Mitigation: For longer documents, use a sliding window approach, chunk the document into sentences or paragraphs, and aggregate results (e.g., average sentence embeddings for document embeddings).
		
	► 1.3 Version Control and Reproducibility:
		• Use Git to track changes in code, configuration files, and documentation. Use clear commit messages and branching strategies (e.g., feature branches).
		• Include a requirements.txt file or a Dockerfile specifying exact package versions (e.g., spaCy==3.4.0, transformers==4.20.0, scikit-learn==1.1.1, etc.) to ensure reproducible environments. Use a virtual environment (e.g., venv, conda).
		• Document the random seed used for any random processes (model training, data splitting) to ensure reproducibility.
		
2. Project Phases and Timeline
This project is structured into six phases, spanning 16 weeks.
Phase 1: Project Setup and Data Ingestion
	► 1.1 Project Kick-Off and Requirements Gathering:
	
		® Define Specific Objectives: Articulate the precise questions the analysis should answer. Examples:
			• Identify key themes and topics across the document corpus.
			• Track sentiment trends related to specific entities or topics over time.
			• Extract all mentions of specific companies, people, or locations.
			• Discover relationships between entities (e.g., co-occurrence, hierarchies).
			• Identify documents similar to a given query document.
		® Scope Definition: Confirm the 1,500 document count, the types of documents (PDF, DOCX, TXT, etc.), and the availability of any existing metadata (creation date, author, source, etc.).
		® Success Criteria: Define how the project's success will be measured. Examples:
			• NER accuracy (precision, recall, F1-score) on a held-out test set.
			• Completeness of data extraction (percentage of relevant entities identified).
			• Actionable insights derived from the analysis 
			• Qualitative assessment of cluster/topic coherence.
		® Project Management: Choose a project management methodology (e.g., Agile with sprints) and tools (e.g., Jira, Trello, Asana, or a simple spreadsheet).
		
	► 1.2 Data Acquisition and Ingestion:
	
		® Gather Documents: Collect all 1,500 documents and confirm access permissions.
		® Implement Ingestion Pipeline: Develop a robust pipeline to handle various file formats:
			• Text & Markdown (.txt, .md): Read directly using Python's file I/O.
			• Word Documents (.docx): Use python-docx2txt to extract text, preserving headers/footers and links if relevant.
			• PDFs (.pdf): Use PyMuPDF (Fitz) for fast and accurate text extraction. Prioritize handling complex layouts (columns, tables, images). If scanned PDFs are present, implement OCR (Optical Character Recognition) using a library like pytesseract (which wraps Tesseract OCR) or a cloud-based OCR service (e.g., AWS Textract, Google Cloud Vision).
			• Other Formats: Utilize libraries like Docling or Unstructured for handling diverse formats (HTML, emails, etc.) under a unified API. If these aren't suitable, develop custom parsers.
		® Storage: Store the raw text of each document in a readily accessible location:
			§ Cloud Storage (Recommended): AWS S3, Google Cloud Storage, or Azure Blob Storage.
			§ Local File System: Use a clear directory structure (e.g., data/raw/).
		® Data Validation:
			§ Check for missing documents (compare the number of ingested files to the expected 1,500).
			§ Check for corrupted files (e.g., files that cannot be opened or parsed).
			§ Ensure all expected content is present (spot-check a sample of documents).
		® Error Handling (from Project 1.txt):
				• Use try-except blocks to handle potential errors (e.g., FileNotFoundError, pdfminer.pdfparser.PDFSyntaxError).
				• Log errors to a file (e.g., ingestion_log.log) with timestamps and details.
				• Implement retry logic for transient errors (e.g., network issues).
		Example:
import os
import logging

logging.basicConfig(filename='ingestion_log.log', level=logging.ERROR)

def ingest_file(file_path):
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"{file_path} does not exist.")
        # ... (rest of the ingestion logic) ...
    except FileNotFoundError as e:
        logging.error(f"FileNotFoundError: {e}")
        # ... (handle the error, e.g., skip the file) ...
    except Exception as e:
        logging.error(f"Error processing {file_path}: {e}")
        # ... (handle other errors) ...
content_copydownload
		
	► 1.3 Data Preprocessing (Integrating Project 1.txt details):
	
		® Encoding: Ensure all text is consistently encoded in UTF-8. Use Python's open() function with the encoding='utf-8' argument. Handle any encoding errors gracefully (e.g., try different encodings, replace invalid characters).
		® Cleaning:
			• Remove extraneous whitespace and newlines (use regular expressions).
			• Handle special characters or encoding errors (replace or remove).
			• Carefully consider removing irrelevant content (e.g., boilerplate text, disclaimers). Only do this if it's consistently identifiable and doesn't contain valuable information. Meticulously document any removals.
		® Normalization (Initial): Convert all text to lowercase. This simplifies later processing and improves consistency.
		® Storage: Store the preprocessed text separately from the raw text (e.g., data/processed/). This preserves the original data and allows for easy comparison.
		® Scalability (from Project 1.txt):
			• Use batch processing to ingest large amount of files.
			
Phase 2: Data Organization and Initial Categorization
	► 2.1 Document Organization:
	
		® Create a logical folder structure based on available metadata (e.g., by date, source, document type, department). This facilitates initial analysis and potential stratified sampling for model evaluation.
		® Assign unique IDs to each document for tracking and referencing (e.g., doc_0001, doc_0002, ...). Store this ID in a metadata file or database.
		
	► 2.2 Initial Categorization (Optional, but Highly Recommended):
	
		® This step provides a baseline for later analysis and can improve the interpretability of clustering results.
		® Option A: Rule-Based Categorization: If predefined categories exist (e.g., "Finance," "HR," "Legal"), implement a rule-based system using:
			• Keywords: Lists of words associated with each category.
			• Regular Expressions: Patterns to match specific phrases or structures.
			• Metadata Fields: Use existing metadata (e.g., "document_type") to assign categories.
		® Option B: Supervised Classification: If labeled data is available (even a small subset, e.g., 100-200 documents), train a text classification model. Options include:
			• scikit-learn with TF-IDF: Use TfidfVectorizer to create features and train a classifier like LogisticRegression or SVC.
			• Simple Transformer Model: Fine-tune a pre-trained transformer (e.g., DistilBERT) using the transformers library. This is likely to be more accurate but requires more computational resources.
		® Option C: Unsupervised Clustering (Exploratory): If no predefined categories or labels exist, use unsupervised clustering techniques (e.g., k-means, HDBSCAN) on document embeddings (generated in Phase 3) to identify potential groupings. This is exploratory and requires careful interpretation. The number of clusters needs to be determined (e.g., using the elbow method for k-means).
		® Documentation: Document the chosen categorization method and its accuracy (if applicable, using metrics like precision, recall, F1-score).
		
	► 2.3 Metadata Extraction:
	
		® If metadata has not been collected, create a database and populate it with any known metadata.
		® Some metadata might be useful for assisting in categorization.
		
Phase 3: NLP Pipeline Implementation 
	► 3.1 Environment Setup:
	
		® Set up a suitable development environment (e.g., Python 3.7+ with a virtual environment, Jupyter Notebooks or a code editor like VS Code).
		® Install necessary libraries (see Technology Stack below).
		® Configure GPU access if available and required (for transformer models). Use libraries like torch or tensorflow to check for GPU availability.
		
	► 3.2 Core NLP Pipeline (spaCy is strongly recommended):
	
		• Tokenization, Lemmatization, Part-of-Speech Tagging, Dependency Parsing: Use spaCy's core pipeline:
import spacy

# Load a suitable spaCy model (en_core_web_sm, en_core_web_md, en_core_web_lg, or en_core_web_trf)
nlp = spacy.load("en_core_web_lg")  # Or a larger/transformer-based model

def process_text(text):
    doc = nlp(text)
    # Access tokens, lemmas, POS tags, dependencies:
    for token in doc:
        print(f"Text: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}, Dep: {token.dep_}")
    return doc

# Process preprocessed text from Phase 1
processed_docs = [process_text(text) for text in preprocessed_texts]
content_copydownload
		• Named Entity Recognition (NER): Use spaCy's built-in NER component:
for doc in processed_docs:
    for ent in doc.ents:
        print(f"Entity: {ent.text}, Type: {ent.label_}")
content_copydownload
		• Stop Word Removal: Use spaCy's is_stop attribute to filter out common stop words:
filtered_tokens = [token for token in doc if not token.is_stop]
content_copydownload
		• Sentence Segmentation: Use spaCy's doc.sents to iterate over sentences:
for sent in doc.sents:
    print(f"Sentence: {sent.text}")
content_copydownload
		• Sentence Embeddings (for Similarity and Clustering): Use the SentenceTransformers library:
from sentence_transformers import SentenceTransformer

# Load a pre-trained sentence embedding model (all-MiniLM-L6-v2 is a good balance of speed and accuracy)
model = SentenceTransformer('all-MiniLM-L6-v2')

def get_sentence_embeddings(sentences):
    embeddings = model.encode(sentences, convert_to_tensor=True) # Use convert_to_tensor=True for GPU
    return embeddings

sentence_embeddings = []
for doc in processed_docs:
  sentences = [sent.text for sent in doc.sents]
  doc_sentence_embeddings = get_sentence_embeddings(sentences)
  sentence_embeddings.append(doc_sentence_embeddings)
content_copydownload
		• Document Embeddings: Calculate document embeddings by averaging the sentence embeddings:
import torch
import numpy as np

document_embeddings = []
for doc_sentence_embeddings in sentence_embeddings:
  # Average the sentence embeddings (on the CPU to avoid GPU memory issues)
  doc_embedding = torch.mean(doc_sentence_embeddings.cpu(), dim=0).numpy()
  document_embeddings.append(doc_embedding)

document_embeddings = np.array(document_embeddings) #Convert to a NumPy array
content_copydownload

Alternative: For potentially richer document representations (if computational resources permit), use a transformer model directly on longer text chunks (up to its maximum sequence length). This avoids the information loss from averaging.
		
		• Entity Linking (Optional, but valuable): If a knowledge base exists (e.g., Wikidata, DBpedia), explore using an entity linking component (e.g., spaCy's EntityLinker or a dedicated library) to link identified entities to their corresponding entries in the knowledge base.
	
	► 3.3 Rule-Based Matching (Supplementing NER):
	
		® Use spaCy's Matcher or PhraseMatcher to define patterns and keywords specific to the domain. This is crucial for:
			• Extracting information that statistical NER might miss (e.g., specific product codes, internal jargon).
			• Finding specific phrases or terminology.
			• Identifying events based on keyword combinations (e.g., "merger with [ORG]").
		® Example:
from spacy.matcher import Matcher

matcher = Matcher(nlp.vocab)

# Example: Match the phrase "merger with" followed by an organization
pattern = [{"LOWER": "merger"}, {"LOWER": "with"}, {"ENT_TYPE": "ORG"}]
matcher.add("MERGER_PATTERN", [pattern])

doc = nlp("The company announced a merger with Acme Corp.")
matches = matcher(doc)
for match_id, start, end in matches:
    span = doc[start:end]
    print(f"Match: {span.text}")
content_copydownload
	► 3.4 Text Classification (Multi-label or Single-label):
	
		® Single-label Classification: If each document belongs to one category (e.g., topic classification), train a classifier:
			• scikit-learn with TF-IDF: (See Phase 2 for details)
			• Transformer Model: Fine-tune a pre-trained transformer (e.g., DistilBERT, RoBERTa) using the transformers library.
		® Multi-label Classification: If documents can belong to multiple categories (e.g., assigning multiple topics), use a multi-label classification approach:
			• One-vs-Rest with Logistic Regression: Use OneVsRestClassifier from scikit-learn with LogisticRegression.
			• Transformer Model with Sigmoid Output: Train a transformer model with a sigmoid activation function on the output layer (instead of softmax).
		® Zero-Shot Classification Consider using a zero-shot approach (using HuggingFace library), if training data is not available.
		
	► 3.5 Sentiment Analysis:
	
		® Use a pre-trained sentiment analysis model:
			• VADER (NLTK): Good for sentence-level sentiment and provides compound, positive, negative, and neutral scores. Simple and fast.
from nltk.sentiment.vader import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()
for doc in processed_docs:
    for sent in doc.sents:
        scores = analyzer.polarity_scores(sent.text)
        print(f"Sentence: {sent.text}, Sentiment: {scores}")
content_copydownload
			• Transformer-based Model (e.g., DistilBERT): For more nuanced and context-aware sentiment analysis. Use a pre-trained model from the transformers library or fine-tune on your own data.
			
		® Aggregate Sentiment Scores: At the document level, calculate:
			• Average sentiment score.
			• Weighted average (weighting by sentence length or importance).
			• Proportion of positive, negative, and neutral sentences.
			
		® Multi-label Sentiment Analysis: If necessary, consider analyzing sentiment with multiple labels (e.g., joy, sadness, anger).
		
	► 3.6 Pipeline Optimization:
	
		○ Profiling: Use Python's cProfile or other profiling tools to identify bottlenecks in the pipeline (which components take the longest time).
		
		○ Batch Processing: Maximize the use of nlp.pipe in spaCy and batch processing in SentenceTransformers to leverage GPU acceleration (if available). This is critical for performance.
# spaCy batch processing
processed_docs = list(nlp.pipe(preprocessed_texts, batch_size=50, n_process=-1)) #n_process=-1 uses all cores

# SentenceTransformers batch processing
embeddings = model.encode(sentences, batch_size=32, convert_to_tensor=True)
content_copydownload
		○ Memory Management: Carefully manage memory usage, especially when dealing with large embeddings.
			§ Use generators or streaming where possible.
			§ Process documents in chunks.
			§ Delete objects that are no longer needed to free up memory.
			§ Consider using smaller models if memory is a major constraint (e.g., en_core_web_sm instead of en_core_web_lg in spaCy).
			
	► 3.7 Model Selection:
	
		○ Start with spaCy models for efficiency. If higher accuracy is needed, move to transformer models.
		○ Always evaluate and compare the performance of different models (using appropriate metrics) before deploying.
		
Phase 4: Semantic Similarity, Topic Modeling, and Clustering
	► 4.1 Cosine Similarity:
	
		○ Calculate cosine similarity between document embeddings to measure the semantic similarity between documents. This is essential for finding related documents and identifying duplicates.
		○ Use libraries like scikit-learn or FAISS (for large-scale similarity search):
from sklearn.metrics.pairwise import cosine_similarity

similarity_matrix = cosine_similarity(document_embeddings)

# Find the most similar document to document 0
most_similar_doc_index = np.argmax(similarity_matrix[0, 1:]) + 1  # Exclude self-similarity
print(f"Most similar document to document 0: {most_similar_doc_index}")
content_copydownload
		○ Normalize vectors before calculating cosine similarity. (This is usually done by default in libraries like scikit-learn)
		
	► 4.2 Clustering (HDBSCAN recommended):
	
		○ Use HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to cluster document embeddings. HDBSCAN is robust to noise and can find clusters of varying shapes and sizes. It doesn't require specifying the number of clusters in advance.
		○ Use cosine similarity as the distance metric.
import hdbscan

clusterer = hdbscan.HDBSCAN(metric='euclidean', min_cluster_size=5, cluster_selection_method='eom') # Use 'eom' for Excess of Mass
cluster_labels = clusterer.fit_predict(document_embeddings)

# Analyze cluster labels (-1 indicates noise points)
print(f"Unique cluster labels: {np.unique(cluster_labels)}")
content_copydownload
		○ Visualize Clusters: Use dimensionality reduction techniques (PCA, UMAP) and scatter plots to visualize the clusters:
import umap
import matplotlib.pyplot as plt

reducer = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine', random_state=42) #n_components = 2 for visualization
embedding_2d = reducer.fit_transform(document_embeddings)

plt.figure(figsize=(10, 8))
plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=cluster_labels, cmap='viridis')
plt.title('Document Clusters (UMAP)')
plt.colorbar()
plt.show()
content_copydownload
		○ Analyze Cluster Content: Examine the documents within each cluster to identify common themes and topics. Create summaries of each cluster (e.g., using word clouds, frequent entities).
		
	► 4.3 Topic Modeling (Alternative/Complementary to Clustering)
	
		○ Consider using topic modeling techniques like LDA (Latent Dirichlet Allocation) or BERTopic (which combines embeddings and clustering).
		○ BERTopic is often preferred as it leverages transformer embeddings and HDBSCAN, aligning well with the previous steps. It provides a more interpretable and often more accurate representation of topics.
from bertopic import BERTopic

topic_model = BERTopic(language="english", calculate_probabilities=True, verbose=True)
topics, probs = topic_model.fit_transform(preprocessed_texts, document_embeddings)

# Get topic information
topic_info = topic_model.get_topic_info()
print(topic_info)

# Visualize top topic terms
topic_model.visualize_barchart()
content_copydownload
		○ Topic modeling can provide an alternative or complementary view to clustering, revealing more nuanced thematic structures.
		
Phase 5: Data Storage, Visualization, and Reporting 
	► 5.1 Data Storage:
	
		® Structured Data Output: Store the results of the NLP pipeline in a structured format:
			• CSV/JSON: For simple data exports and interoperability.
			• Database (Recommended): Use a relational database (e.g., PostgreSQL, MySQL) or a NoSQL database (e.g., MongoDB) for scalability, querying, and data integrity. Choose the database based on your needs and the structure of your data.
				Store:
					- Document metadata (ID, source, original category, file path, etc.).
					- Extracted entities (entity text, type, start/end positions in the original text, links to knowledge base IDs if applicable).
					- Sentiment scores (overall document sentiment, sentence-level sentiment).
					- Topic assignments (from clustering or topic modeling).
					- Classification labels (if applicable).
					- Sentence and document embeddings (can be stored in a separate vector database like FAISS or Annoy for efficient similarity search).
					
		® Vector Index (Optional): For efficient similarity search (finding similar documents quickly), consider using a specialized vector index like FAISS or Annoy.
		
	► 5.2 Visualization:
	
		® Create visualizations to communicate the findings effectively to both technical and non-technical audiences.
		® spaCy's displaCy: For visualizing dependency parses and named entities within the context of the text.
from spacy import displacy

doc = nlp("Apple is looking at buying U.K. startup for $1 billion")
displacy.serve(doc, style="ent")  # Or style="dep" for dependency parsing
content_copydownload
		® Matplotlib/Seaborn: For general-purpose charts (histograms, scatter plots, bar charts, line graphs, heatmaps).
		® Word Clouds: To visualize the most frequent terms in the corpus or within specific clusters/topics. Use the wordcloud library.
		® Network Graphs: To illustrate relationships between entities (e.g., co-occurrence networks). Use libraries like NetworkX and pyvis (for interactive graphs).
		® Interactive Plots (Optional): Use libraries like Plotly or Bokeh for interactive visualizations that allow users to explore the data.
		® Dimensionality Reduction Plots: PCA/UMAP plots of document embeddings, colored by cluster or topic (see Phase 4).
		® Bar Charts: Top N entities, document counts per category, etc.
		® Line Graphs: Sentiment over time (if time-series data is available).
		® Confusion Matrix For displaying classification model results.
		
	► 5.3 Reporting:
	
		® Jupyter Notebook (Recommended): Combine code, visualizations, and narrative text in a Jupyter Notebook to create a comprehensive and reproducible report. This allows for easy sharing and collaboration.
		® Automated Report Generation (Optional): Use tools like NBConvert or Quarto to convert the Jupyter Notebook to HTML, PDF, or other formats for wider distribution.
		® Dashboard (Optional): Consider building a dashboard (e.g., using Streamlit or Dash) to provide interactive access to the data and insights. This is particularly useful for stakeholders who want to explore the data themselves.
		® Key Findings Summary: Clearly articulate the key findings and insights, directly addressing the initial business questions defined in Phase 1.
		® Data-Driven Narrative: Present the results in a clear and concise narrative, supported by data and visualizations. Avoid technical jargon where possible.
		® Tailor Reporting: Customize reports for different audiences (e.g., a high-level summary for executives, a detailed technical report for the data science team).
		
	► 5.4 Data Dictionary
	
		® Provide a section on the different terms and acronyms used.
		
Phase 6: Review, Refinement, and Deployment 
	► 6.1 Performance Evaluation:
	
		® Evaluate the accuracy of the NLP pipeline components (NER, classification, sentiment analysis) using appropriate metrics:
			• NER: Precision, recall, F1-score (at the entity level).
			• Classification: Precision, recall, F1-score, accuracy (for single-label); micro-average, macro-average, weighted-average F1-score (for multi-label).
			• Sentiment Analysis: Accuracy (if comparing to ground truth labels), correlation with human judgments.
		® Assess the quality of the clustering/topic modeling results using:
			• Coherence Scores: (e.g., c_v, u_mass for topic models).
			• Silhouette Score: (for clustering, although less reliable for high-dimensional data).
			• Qualitative Evaluation: Examine the content of clusters/topics to ensure they are meaningful and interpretable.
		® Gather feedback from stakeholders on the usefulness of the insights and the clarity of the reporting.
		
	► 6.2 Refinement:
	
		® Based on the evaluation and feedback, refine the pipeline:
			• Adjust parameters (e.g., clustering thresholds, model hyperparameters).
			• Improve training data (e.g., add more labeled examples, correct errors).
			• Add or remove components (e.g., try different NER models, add entity linking).
			• Address any identified errors or biases in the analysis.
			
	► 6.3 Deployment:
	
		® Deploy the pipeline for ongoing use:
			• Create a production-ready version of the code (clean up, refactor, add error handling).
			• Set up a system for processing new documents (e.g., a scheduled batch job, a real-time API).
			• Integrate the results into downstream applications or workflows (e.g., a dashboard, a search engine, a reporting system).
			
		® Create thorough documentation for deployment.
		
	► 6.4 Maintenance:
	
		® Establish a plan for ongoing maintenance and updates:
			• Monitor the performance of the pipeline over time (track key metrics).
			• Retrain models periodically (e.g., every month, every quarter) to adapt to changes in the data distribution.
			• Update the pipeline to address new business requirements or incorporate new NLP techniques.
			• Create thorough documentation for maintenance.
			
Technology Stack:
	• Programming Language: Python (3.7+)
	• NLP Libraries:
		- spaCy (core NLP pipeline, NER, dependency parsing)
		- NLTK (VADER sentiment analysis, general NLP utilities)
		- SentenceTransformers (sentence and document embeddings)
		- Transformers (Hugging Face) (fine-tuning transformer models)
	• Clustering:
		- HDBSCAN (primary clustering algorithm)
		- scikit-learn (k-means, PCA, cosine similarity)
	• Topic Modeling:
		- Gensim (LDA)
		- BERTopic (recommended)
	• Visualization:
		- Matplotlib
		- Seaborn
		- Plotly (optional, for interactive plots)
		- Bokeh (optional, for interactive plots)
		- displaCy (spaCy's visualization tool)
		- NetworkX (network graphs)
		- pyvis (interactive network graphs)
		- wordcloud
	• Databases:
		- PostgreSQL (recommended relational database)
		- MySQL (alternative relational database)
		- MongoDB (NoSQL database, good for flexible schemas)
		- FAISS (vector index for efficient similarity search)
		- Annoy (alternative vector index)
	• Reporting:
		- Jupyter Notebook (primary reporting tool)
		- NBConvert (for exporting notebooks to other formats)
		- Quarto (alternative to NBConvert)
		- Streamlit (optional, for building dashboards)
		- Dash (optional, for building dashboards)
	• Project Management:
		- Jira (optional, for larger teams)
		- Asana (optional, for task management)
		- Trello (optional, for simpler task management)
	• Other:
		- OCR: Pytesseract
		- Regex
	
High-Level Flowchart (from Project 1.txt, updated):
[Data Ingestion (Multiple Formats)] --(UTF-8 Text)--> [Preprocessing (Cleaning, Normalization, Tokenization, Lemmatization, Stop Word Removal)]
  -- (Cleaned Tokens) --> [Feature Extraction (Sentence Embeddings, Document Embeddings, TF-IDF (optional))]
  -- (Numerical Features) --> [Modeling (NER, Sentiment Analysis, Text Classification, Clustering, Topic Modeling)]
  -- (Model Outputs) --> [Evaluation (Metrics, Visualization)] -- (Insights) --> [Deployment (API, Batch Processing)]
  -- (Structured Data) --> [Storage (Database, CSV/JSON)]
