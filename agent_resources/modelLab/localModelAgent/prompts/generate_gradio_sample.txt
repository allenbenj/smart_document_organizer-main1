Generate Gradio Sample from Standalone Inference Script

Your task is write a python sample in gradio to demonstrate the usage of an AI model in GUI.
The code should be personalized and adapted not only based on the model inference sample,
but also considering the chat context and user's request.

Parameters:
- scriptDir: `test_gradio_sample.py` is located here.
- outputDir: `inference_script.py` is located here. You should generate gradio sample here.

Requirements:
IMPORTANT: Reuse the exact same model path as in the above code, defined as a string. Do not do any model conversion.
you MUST specify path to onnx model to use in your code, and NEVER obtain the model path or any other values from environment variables, since your code will be run and tested in full automation without setting any environment variable beforehand.
Load this model before gradio code.

Instead of using the input sample in the script, the gradio sample should take the input from appropriate input widgets for each input property.

There is a button that user can click to run the inference.

The ouput is shown via output widgets. If a widget exists for an output type, prefer it over simple text output.

In addition to model output, include an extra textbox to show the elapsed time after running a sample.

IMPORTANT: Include a Model Card section at the top of the Gradio interface with the following information displayed using Markdown or HTML:
- Model Name: Extract from the model metadata or use the model filename
- Execution Provider (EP): Display the execution provider being used (e.g., CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- Model Path: Extract and display the actual model path or model folder used in the inference script. This should reflect the exact path string defined in the code.
- Input & Output: Provide a brief description of the model's input and output specifications (dimensions, data types, description)
This model card should be displayed in a clear, readable format using gradio.Markdown() or gradio.HTML() component at the beginning of the interface. All information should be extracted from the inference script and model metadata.

IMPORTANT: You must create only one Gradio API. This API must be specified with argument `api_name="predict"`, be it in `gradio.Interface()`, `button.click()`, `gradio.api()` or any other gradio code where `api_name` can be passed.

IMPORTANT: DO NOT generate lazy loading pattern sample, which means only load model when first time generate output. This is because loading model on a target execution provider and device is an essential step when testing sample.

Support for client mode:
Add argparse support with an optional `--client` flag. When `--client` is passed:
- Import `from gradio_client import Client, handle_file`
- Instead of launching the gradio server, create a client with `Client("http://localhost:7860")`
- Use `client.predict()` with one of the examples from the gradio interface definition
- IMPORTANT: Make sure to to pass `api_name="/predict"` (with slash) parameter to `client.predict()`, but do NOT pass `fn_index` parameter to it
- Print the result and exit
- IMPORTANT: Do NOT use try/except clauses in client mode code. Let all exceptions propagate naturally - the exceptions and process return codes are crucial for automatic debugging and fixing

IMPORTANT: When launching the Gradio server, ALWAYS call .launch() with show_error=True to enable verbose error reporting for debugging:
```python
demo.launch(show_error=True)
```

IMPORTANT: Make sure to expose only one Gradio API in server mode to avoid confusing the client.

If the sample includes image inputs, remember to display the image(s) as output, since users may use image URLs as input and may not see the image content in the UI.

IMPORTANT: the input examples MUST be defined as a global variable and used in both server mode and client mode, to maintain consistency.

Your code will be saved to a gradio script file. Make sure it will work with:
`python gradio_sample.py` (server mode)
`python gradio_sample.py --client` (client mode)

IMPORTANT: when creating `ort.InferenceSession`, you must ALWAYS pass `sess_options` argument, after setting `sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED` to fix a known issue.

Steps:
- Refer to `inference_script.py`, as you should write Gradio sample based on it; only when `inference_script.py` import `sample_winml` module and calls `sample_winml.register_execution_providers` should you read `## how_to_use_winml` section for extra instructions;
- Double check `widget_data` field of the sample json file. If `widget_data` includes example inputs, add them to the gradio sample for user to select from.
- Generate `gradio_sample.py` following above instructions; If user provided any hints about preferred input/output styles or behaviors, reflect them in the Gradio UI when possible;
- Use `test_gradio_sample.py` in `script_dir` to test gradio sample. Run `$env:PYTHONIOENCODING='utf-8'; python test_gradio_sample.py gradio_sample.py` to test it;
- If you encounter errors in test, analyze and fix them. Make sure to:
    1. Make sure all requirements in this document are met.
    2. Keep using the exact same ONNX model path as in the previous code, defined as a string. Do not change the model or do any conversion. you MUST specify path to onnx model to use in your code, and NEVER obtain the model path or any other values from environment variables, since your code will be run and tested in full automation without setting any environment variable beforehand.
    3. Do not rewrite the whole logic unnecessarily. Only fix the issues so that the script runs correctly.
    4. Ensure all gradio widgets work properly:
        - Inputs should come from widgets as specified in `widget_data`.
        - If `widget_data` includes example inputs, keep them as selectable options.
        - There must be a button to run inference.
        - Outputs must be displayed with the correct widgets (image, audio, textbox, etc.).
        - Add a textbox showing the elapsed time after inference.
- Run the fixing-testing loop for AT MOST 5 times. IMPORTANT: do not tweak input examples or test code smartly to bypass the failed cases; instead focus on fixing issues in inference code.