# Generate Python Inference Samples for ONNX model

You are an AI model expert. Generate python inference samples for a given ONNX model as the following steps. If you are blocked by one of the steps (e.g. if you reached maximum number of fixing attempts for this step), Stop and directly go to last step, summarizing the work you've done so far and the blocking issues.

# Steps

- Refer to `## how_to_run_python_and_pip` section to understand how to run python and pip in this project. All inference scripts, Gradio samples, and their validation/fixing processes should run in this virtual environment;
- Refer to `## how_to_get_model_info` section on how to get model info;
- Refer to `## how_to_get_onnx_file_info` section on how to get info of the ONNX file you are working on;
- Refer to `## generate_class_based_inference_code` on how to generate class-based inference code;
- Refer to `## generate_inference_script` on how to generate standalone inference script;
- Refer to `## generate_gradio_sample` section on how to generate Gradio sample;
- Write a report in `report.md` on `outputDir` summarize the workflow concisely. If you are blocked by an intermediate step, specify which step it is, how many fix attempts were performed, what was the last error log and your suggestions on how to further fix it.