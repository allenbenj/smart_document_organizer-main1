You are given the code cells of a Jupyter notebook, the target executionProvider and the Onnx model path(s) or model folder. Convert the notebook into a single, runnable Python inference sample.

Parameters:
- Notebook code (only code cells) follows: `inference_sample.ipynb` located in `outputDir`.
- Target executionProvider: parameter `executionProvider`.
- Target device type: parameter `deviceType`. E.g. OrtHardwareDeviceType.NPU
- Model paths (for non-LLM): parameter `modelPaths`. A JSON object mapping model names to their ONNX file paths. The keys are model identifiers and values are absolute paths. Examples:
    - Single model: `{"model": "/path/to/model.onnx"}`
    - Multiple models (e.g., encoder-decoder): `{"encoder": "/path/to/encoder.onnx", "decoder": "/path/to/decoder.onnx"}`
- Model folder (for LLM): parameter `modelDir`. The directory containing the ONNX file(s).
- Output folder: parameter `outputDir`, which your sample will generate in.

Requirements:
- A verified notebook (e.g. `inference_sample.ipynb`) is available. Use this notebook as the primary reference source for inference logic (preprocessing, input/output handling, postprocessing, etc.).
- Remove all code that will not be executed in the given `executionProvider`, and also remove any variable definitions that are not necessary for execution.
- Refer to `## how_to_use_winml` section for extra instructions;
    - Important: There is an essential difference between using `add_ep_for_device` and directly specifying ep in the providers argument of `ort.InferenceSession`. Do NOT replace the `add_ep_for_device` logic with the providers argument. The `add_ep_for_device` approach is required to ensure correct device selection and execution. Do not modify, substitute, or remove this logic under any circumstances.
    - Important: ALWAYS `register_execution_providers()` to support ep. This is required before the inference session is created.
    - IMPORTANT: NEVER search or copy `winml.py` or tweak to rename them!! You have been provided `sample_winml.py` which you must refer and complete the code.
- Move all import statements to the beginning of the file.
- Read the notebook and replace the model path or model folder:
    - For non-LLM models, use `modelPaths` parameter. Iterate through the JSON object and use each key-value pair to replace the corresponding model paths in the code. For example:
        - If `modelPaths` has key "model", use `modelPaths["model"]` as the ONNX file path
        - If `modelPaths` has keys "encoder" and "decoder", use them as the respective paths
    - For LLM models, use `modelDir` parameter as the model folder path.

Return only the Python code for the final inference sample.

IMPORTANT: ALWAYS use the exact same ONNX model path or model folder as given above, defined as a string. Do NOT change the model or do any conversion. Do NOT use another model path or model folder.

IMPORTANT: In the above notebook, you may see lines like:
```python
ExecutionProvider="QNNExecutionProvider"
...
add_ep_for_device(session_options, ExecutionProvider, ort.OrtHardwareDeviceType.NPU)
```
These lines were for demo purpose and are expected to be changed manually by user.
In the code you are about to write, you MUST replace them with
```python
ExecutionProvider="{executionProvider}"
...
add_ep_for_device(session_options, ExecutionProvider, ort.{deviceType})
```

IMPORTANT: When creating `ort.InferenceSession`, you must ALWAYS pass `sess_options` argument, after setting `sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED` to fix a known issue.

Steps:
- Generate standalone `inference_script.py` in `outputDir` following above instructions;
- Install missing dependencies, and run `python inference_script.py` to test it on same samples;
- If you encounter errors in test, analyze and fix them. Make sure to:
    1. Ensure the script is executable and handles edge cases properly
    2. Maintain proper error handling
- Run the fixing-testing loop for AT MOST 5 times. IMPORTANT: do not tweak input examples or test code smartly to bypass the failed cases; do not tweak model because the task is generate sample for GIVEN MODEL!! Instead focus on fixing issues in inference code.